<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DNN Training on Haoran You</title>
    <link>http://localhost:1313/tags/dnn-training/</link>
    <description>Recent content in DNN Training on Haoran You</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Oct 2020 22:23:07 -0500</lastBuildDate>
    
	<atom:link href="http://localhost:1313/tags/dnn-training/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Efficient DNN Training</title>
      <link>http://localhost:1313/post/efficient-training/</link>
      <pubDate>Fri, 02 Oct 2020 22:23:07 -0500</pubDate>
      
      <guid>http://localhost:1313/post/efficient-training/</guid>
      <description>Efficient DNN Training Summary Model compression has been extensively studied for light-weight inference, popular means includes network pruning, weight factorization, network quantization, and neural architecture search among many others. On the other hand, the literature on efficient training appears to be much sparser, DNN training still requires us to fully train the over-parameterized neural network. Here we focus on reducing total training times and training energy cost, aiming at the deployment on resource-constrainted platforms, e.</description>
    </item>
    
    <item>
      <title>DNN Training Stages Understanding</title>
      <link>http://localhost:1313/post/dnn-training/</link>
      <pubDate>Sat, 21 Mar 2020 22:23:07 -0500</pubDate>
      
      <guid>http://localhost:1313/post/dnn-training/</guid>
      <description>Recent works show that DNN training undergoes different stages, each stage shows different effects given a hyper-parameter setting and therefore entails detailed explaination. Below I aims to analyze and share the deep understanding of DNN training, especially from the following three perspectives:
 On the optimization and generalization perspective On the frequency domain perspective What happens during the early phase of DNN training  On the Optimization and Generalization Perspective The connection between optimization and generalization of deep neural networks (DNN) is not fully understood.</description>
    </item>
    
    <item>
      <title>[ICLR 2020] Drawing Early-Bird Tickets: Towards More Efficient Training of Neural Networks</title>
      <link>http://localhost:1313/publication/early-bird/</link>
      <pubDate>Wed, 02 Oct 2019 18:03:59 -0500</pubDate>
      
      <guid>http://localhost:1313/publication/early-bird/</guid>
      <description>Accepted as spotlight oral paper! Abstract: (Frankle &amp;amp; Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.</description>
    </item>
    
  </channel>
</rss>