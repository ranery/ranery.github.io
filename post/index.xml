<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Haoran You</title>
    <link>http://localhost:1313/post/</link>
    <description>Recent content in Posts on Haoran You</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Oct 2020 22:23:07 -0500</lastBuildDate>
    
	<atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Efficient DNN Training</title>
      <link>http://localhost:1313/post/efficient-training/</link>
      <pubDate>Fri, 02 Oct 2020 22:23:07 -0500</pubDate>
      
      <guid>http://localhost:1313/post/efficient-training/</guid>
      <description>Efficient DNN Training Summary Model compression has been extensively studied for light-weight inference, popular means includes network pruning, weight factorization, network quantization, and neural architecture search among many others. On the other hand, the literature on efficient training appears to be much sparser, DNN training still requires us to fully train the over-parameterized neural network.</description>
    </item>
    
    <item>
      <title>DNN Training Stages Understanding</title>
      <link>http://localhost:1313/post/dnn-training/</link>
      <pubDate>Sat, 21 Mar 2020 22:23:07 -0500</pubDate>
      
      <guid>http://localhost:1313/post/dnn-training/</guid>
      <description>Recent works show that DNN training undergoes different stages, each stage shows different effects given a hyper-parameter setting and therefore entails detailed explaination. Below I aims to analyze and share the deep understanding of DNN training, especially from the following three perspectives:</description>
    </item>
    
    <item>
      <title>Lottery Ticket Hypothesis</title>
      <link>http://localhost:1313/post/lottery-ticket/</link>
      <pubDate>Sun, 12 Jan 2020 15:24:07 -0600</pubDate>
      
      <guid>http://localhost:1313/post/lottery-ticket/</guid>
      <description>Lottery Ticket Hypothesis  A randomly-initialized, dense neural network contains a subnetwork that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.</description>
    </item>
    
    <item>
      <title>Cultivate Good Research Taste</title>
      <link>http://localhost:1313/post/research_taste/</link>
      <pubDate>Thu, 02 Jan 2020 22:01:18 -0600</pubDate>
      
      <guid>http://localhost:1313/post/research_taste/</guid>
      <description>This post is supposed to be my reflect about cultivating good research taste as an individual researcher, and should always be maintained and reviewed!  
Updates  01/02/2020 Today I find one paper I have criticized got accepted as an oral presentation, I was dismissive at the first glance since one can easily understand how it suppose to work technically and further regarded it as granted.</description>
    </item>
    
    <item>
      <title>Hermann Park</title>
      <link>http://localhost:1313/post/hermann_park/</link>
      <pubDate>Tue, 24 Dec 2019 20:40:26 -0600</pubDate>
      
      <guid>http://localhost:1313/post/hermann_park/</guid>
      <description>Before applying for Ph.D., I heard that Rice University is of highest happiness index. I assured this is the case for undergraduates while not applied to Ph.D. students since environment makes no difference for those living in office (XD).</description>
    </item>
    
    <item>
      <title>Hello World</title>
      <link>http://localhost:1313/post/hello-world/</link>
      <pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/post/hello-world/</guid>
      <description>This is first post!</description>
    </item>
    
  </channel>
</rss>