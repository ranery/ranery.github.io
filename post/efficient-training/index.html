<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Efficient DNN Training - Haoran You</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta property="og:title" content="Efficient DNN Training" />
<meta property="og:description" content="Efficient DNN Training Summary Model compression has been extensively studied for light-weight inference, popular means includes network pruning, weight factorization, network quantization, and neural architecture search among many others. On the other hand, the literature on efficient training appears to be much sparser, DNN training still requires us to fully train the over-parameterized neural network." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:50269/post/efficient-training/" />
<meta property="article:published_time" content="2020-10-02T22:23:07-05:00"/>
<meta property="article:modified_time" content="2020-10-02T22:23:07-05:00"/>

	<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Efficient DNN Training"/>
<meta name="twitter:description" content="Efficient DNN Training Summary Model compression has been extensively studied for light-weight inference, popular means includes network pruning, weight factorization, network quantization, and neural architecture search among many others. On the other hand, the literature on efficient training appears to be much sparser, DNN training still requires us to fully train the over-parameterized neural network."/>

	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">
	<link rel="shortcut icon" href="/favicon.ico">
	
<script async src="https://www.googletagmanager.com/gtag/js?id=G-19W0TQGWMW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-19W0TQGWMW');
</script>
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="Haoran You" rel="home">
				<div class="logo__title">Haoran You</div>
				<div class="logo__tagline">Ph.D. student at Georgia Tech EIC Lab</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">Bio</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/">Blogs</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/projects/">Projects</a>
		</li>
	</ul>
</nav>

	</div>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} })
    </script>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Efficient DNN Training</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2020-10-02T22:23:07">2020-10-02</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/research" rel="category">Research</a></span>
</div>
</div>
		</header>
<div class="content post__content clearfix">
			

<h2 id="efficient-dnn-training-summary">Efficient DNN Training Summary</h2>

<p><meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@Haoran You">
<meta name="twitter:creator" content="@Haoran You">
<meta name="twitter:title" content="Efficient DNN Training">
<meta name="twitter:description" content="Introduction to the three most recently works putting huge efforts toward the efficient training goal.">
<meta name="twitter:image" content="https://github.com/ranery/ranery.github.io/blob/master/img/DNN.png"></p>

<p>Model compression has been extensively studied for light-weight inference, popular means includes network pruning, weight factorization, network quantization, and neural architecture search among many others. On the other hand, the literature on efficient training appears to be much sparser, DNN training still requires us to fully train the over-parameterized neural network.
Here we focus on reducing total training times and training energy cost, aiming at the deployment on resource-constrainted platforms, e.g., FPGAs, ASICs, Mobile and IoT devices. Below are three most recently works putting huge efforts toward the efficient training goal.</p>

<p><div align=center>
    <img src="/img/DNN_source.png" alt="Trulli" style="width:50%;vertical-align: middle">
    <span> <p></p> General DNN Training Scheme.</span>
</div>
<p></p></p>

<hr />

<ul>
<li><p><strong><a href="https://arxiv.org/abs/1910.13349">E^2 Train (NeurIPS 2019)</a></strong>: To conduct more energy-efficient training of CNNs, so as to enable on-device training. This paper strive to reduce the energy cost during training, by dropping unnecessary computations from three complementary levels: stochastic mini-batch dropping <strong>on the data level</strong>; selective layer update <strong>on the model level</strong>; and sign prediction for low-cost, low-precision back-propagation, <strong>on the algorithm level</strong>.
<div align=center>
<img src="/img/E2Train.png" hspace="0" alt="Trulli" style="width:75%">
</div>
<p style = "margin:10px"></p>
<div align=left>
<figcaption style="width:97%;float:center;">
Fig.1 - An illustration of E^2 Train framework. On the data level, we adopt stochastic mini-batch dropping to aggressively reduce the training cost by, letting it see less mini-batches; On the model level, we dynamically skipping a subset of layers during both feed-forward and back-propagation; On the algorithm level, we propose a novel predictive sign gradient descent (PSG) algorithm, which predicts the sign of gradients using low-cost bit-level predictors, thereby completely bypassing the costly full-gradient computation.
</figcaption>
</div>
<br>
Benefit from aforementioned three techiniques, we are able to achieve <strong>over 80% total training energy cost savings</strong> measured by FPGA. For example, when training ResNet-74 on CIFAR-10, we achieve aggressive energy savings of &gt;90% and &gt;60%, while incurring a top-1 accuracy loss of only about 2% and 1.2%, respectively. When training ResNet-110 on CIFAR-100, an over 84% training energy saving is achieved without degrading inference accuracy.</p>

<pre><code>Code avaialble at: https://github.com/RICE-EIC/E2Train
</code></pre></li>
</ul>

<hr />

<ul>
<li><p><strong><a href="https://arxiv.org/abs/1909.11957">EB Train (ICLR 2020 Spotlight)</a></strong>: This paper bridges the gap between <a href="https://ranery.github.io/post/lottery-ticket/">Lottery Ticket Hypothesis</a> and efficient training goal. Since the identification of winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits, this paper discover for the first time that the winning tickets can be identified <strong>at the very early training stage (Early-Bird (EB) tickets)</strong> via low-cost training schemes, and propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Therfore, we can leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy.
<div align=justify>
<img src="/img/EBTrain.png" alt="Trulli" hspace="100" style="width:65%">
</div>
<div align=left>
<figcaption style="width:97%;float:center;">
Fig.2 - A high-level overview of the difference between EB Train and existing progressive pruning and training.
In particular, the progressive pruning and training scheme adopts a three-step routine of 1) training a large and dense model, 2) pruning it, and 3) then retraining the pruned model to restore performance, and these three steps can be iterated.
The first step often dominates (e.g., occupy 75% training FLOPs) in terms of training energy and time costs.
While EB Train replaces the aforementioned steps 1 and 2 with a lower-cost step of detecting the EB tickets.
</figcaption>
</div>
<br>
Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets, and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient  training via EB tickets can <strong>achieve up to 4.7× energy savings</strong> while maintaining comparable or even better accuracy, demonstrating a promising and easily adopted methodfor tackling the often cost-prohibitive deep network training.</p>

<pre><code>Code avaialble at: https://github.com/RICE-EIC/Early-Bird-Tickets
</code></pre></li>
</ul>

<hr />

<ul>
<li><p><strong><a href="404">FracTrain (NeurIPS 2020)</a></strong>: This paper proposes FracTrain which integrates: 1) progressive fractional quantization which gradually increases the bitwidth of activations, weights, and gradients that will not reach the precision of SOTA static quantized DNN training until the final training stage, and 2) dynamic fractional quantization which assigns bitwidths to both the activations and gradients of each layer in an input-adaptive manner, for only &ldquo;fractionally&rdquo; updating layer parameters.
<p style = "margin:10px"></p>
<div align=justify>
<img src="/img/fractrain.png" alt="Trulli" hspace="40" style="width:85%">
</div>
<p style = "margin:10px"></p>
<div align=left>
<figcaption style="width:97%;float:center;">
Fig.3 - <strong>(a)</strong> A high-level view of the proposed PFQ vs. SOTA low-precision training, where PFQ adopts a four-stage precision schedule to gradually increase precision of weights, activations, gradients, and errors up to that of the static baseline which here employs 8-bit for both the forward and backward paths, denoted as FW-8/BW-8, and <strong>(b)</strong> the corresponding training loss trajectory.
</figcaption>
</div>
<br>
Extensive simulations and ablation studies (six models, four datasets, and three training settings including standard, adaptation, and fine-tuning) validate the effectiveness of FracTrain in reducing computational cost and hardware-quantified energy/latency of DNN training while achieving a comparable or better (-0.12% ~ +1.87%) accuracy. For example, when training ResNet-74 on CIFAR-10, FracTrain achieves <strong>77.6% and 53.5% computational cost and training latency savings</strong>, respectively, compared with SOTA baseline, while achieving a comparable (-0.07%) accuracy.</p>

<pre><code>Code will be available soon.
</code></pre></li>
</ul>

<hr />

<ul>
<li><p><strong><a href="404">ShiftAddNet (NeurIPS 2020)</a></strong>: This paper presented ShiftAddNet, whose main inspiration is drawn from a common practice in energy-efficient hardware implementation, that is, multiplication can be instead performed with additions and logical bit-shifts. We leverage this idea to explicitly parameterize deep networks in this way, yielding a new type of deep network that involves only bit-shift and additive weight layers.
<p style = "margin:10px"></p>
<div align=justify>
<img src="/img/shiftadd.png" alt="Trulli" hspace="85" style="width:75%">
</div>
<p style = "margin:10px"></p>
<div align=center>
<figcaption style="width:97%;float:center;">
Fig.4 - Overview structure of ShiftAddNet.
</figcaption>
</div>
<br>
This hardware-inspired ShiftAddNet immediately lead to both energy-efficient inference and training, without compromising the expressive capacity compared to standard DNNs. The two complementary operations types (bit-shift and add) additionally enable finer-grained control of the model&rsquo;s learning capacity, leading to more flexible trade-off between accuracy and (training) efficiency.
Compared to existing DNNs or other multiplication-less models, ShiftAddNet aggressively <strong>reduces over 80% the hardware-quantified energy cost</strong> of DNN training and inference, while offering comparable or better accuracies.</p>

<pre><code>Code will be available soon.
</code></pre></li>
</ul>

<p><br>
<br></p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/dnn-training/" rel="tag">DNN Training</a></li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/shiftaddnet/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">[NeurIPS 2020] ShiftAddNet: A Hardware-Inspired Deep Network </p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/hiking_kennesaw/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">Hiking at Kennesaw Mountain</p></a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 Haoran You.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
    </script>
    
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="/js/custom.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>

<script data-no-instant>document.write('<script src="/livereload.js?port=50269&mindelay=10"></' + 'script>')</script></body>
</html>