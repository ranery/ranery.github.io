<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications on Haoran You</title>
    <link>http://localhost:1313/publication/</link>
    <description>Recent content in Publications on Haoran You</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Sep 2020 18:03:59 -0600</lastBuildDate>
    
	<atom:link href="http://localhost:1313/publication/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>[NeurIPS 2020] ShiftAddNet: A Hardware-Inspired Deep Network </title>
      <link>http://localhost:1313/publication/shiftaddnet/</link>
      <pubDate>Tue, 22 Sep 2020 18:03:59 -0600</pubDate>
      
      <guid>http://localhost:1313/publication/shiftaddnet/</guid>
      <description>Accepted as NeurIPS 2020 regular paper! Abstract: Multiplication (e.g., convolution) is arguably a cornerstone of modern deep neural networks (DNNs). However, intensive multiplications cause expensive resource costs that challenge DNN deployment on resource-constrained edge devices, driving several attempts for multiplication-less deep networks. This paper presented ShiftAddNet, whose main inspiration is drawn from a common practice in energy-efficient hardware implementation, that is, multiplication can be instead performed with additions and logical bit-shifts.</description>
    </item>
    
    <item>
      <title>[NeurIPS 2020] FracTrain: Fractionally Squeezing Bit Savings Both Temporally and Spatially for Efficient DNN Training </title>
      <link>http://localhost:1313/publication/fractrain/</link>
      <pubDate>Tue, 22 Sep 2020 18:03:59 -0500</pubDate>
      
      <guid>http://localhost:1313/publication/fractrain/</guid>
      <description>Accepted as NeurIPS 2020 regular paper! Abstract: Recent breakthroughs in deep neural networks (DNNs) have fueled a tremendous demand for intelligent edge devices featuring on-site learning, while the practical realization of such systems remains a challenge due to the limited resources available at the edge and the required massive training costs for state-of-the-art (SOTA) DNNs. As reducing precision is one of the most effective knobs for boosting training time/energy efficiency, there has been a growing interest in low-precision DNN training.</description>
    </item>
    
    <item>
      <title>[ECCV 2020] HALO: Hardware-Aware Learning to Optimize</title>
      <link>http://localhost:1313/publication/l2o/</link>
      <pubDate>Sat, 02 May 2020 18:03:59 -0500</pubDate>
      
      <guid>http://localhost:1313/publication/l2o/</guid>
      <description>Accepted as ECCV 2020 regular paper! Abstract: There has been an explosive demand for bringing machine learning (ML) powered intelligence into numerous Internet-of-Things (IoT) devices. However, the effectiveness of such intelligent functionality requires in-situ continuous model adaptation for adapting to new data and environments, while the on-device computing and energy resources are usually extremely constrained. Neither traditional hand-crafted (e.g., SGD, Adagrad, and Adam) nor existing meta optimizers are specifically designed to meet those challenges, as the former requires tedious hyper-parameter tuning while the latter are often costly due to the meta algorithmsâ€™ own overhead.</description>
    </item>
    
    <item>
      <title>[ISCA 2020] SmartExchange: Trading Higher-cost Memory Storage/Access for Lower-cost Computation</title>
      <link>http://localhost:1313/publication/smart-exchange/</link>
      <pubDate>Sun, 02 Feb 2020 18:03:59 -0500</pubDate>
      
      <guid>http://localhost:1313/publication/smart-exchange/</guid>
      <description>Accepted as ISCA 2020 regular paper! Abstract: We present SmartExchange, an algorithm-hardware co-design framework to trade higher-cost memory storage/access for lower-cost computation, for energy-efficient inference of deep neural networks (DNNs). We develop a novel algorithm to enforce a specially favorable DNN weight structure, where each layerwise weight matrix can be stored as the product of a small basis matrix and a large sparse coefficient matrix whose non-zero elements are all power-of-2.</description>
    </item>
    
    <item>
      <title>[ICLR 2020] Drawing Early-Bird Tickets: Towards More Efficient Training of Neural Networks</title>
      <link>http://localhost:1313/publication/early-bird/</link>
      <pubDate>Wed, 02 Oct 2019 18:03:59 -0500</pubDate>
      
      <guid>http://localhost:1313/publication/early-bird/</guid>
      <description>Accepted as spotlight oral paper! Abstract: (Frankle &amp;amp; Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.</description>
    </item>
    
    <item>
      <title>[IEEE TNNLS] Bayesian Cycle-Consistent Generative Adversarial Networks via Marginalizing Latent Sampling</title>
      <link>http://localhost:1313/publication/bayesian-cyclegan/</link>
      <pubDate>Wed, 02 Oct 2019 18:03:59 -0500</pubDate>
      
      <guid>http://localhost:1313/publication/bayesian-cyclegan/</guid>
      <description>Accepted as IEEE TNNLS regular paper! Abstract: Recent techniques built on generative adversarial networks (GANs), such as cycle-consistent GANs, are able to learn mappings among different domains built from unpaired data sets, through min-max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for interdomain mappings.</description>
    </item>
    
  </channel>
</rss>